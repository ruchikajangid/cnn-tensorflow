{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 102_Flowers_Dataset_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Srtxli_m69W",
        "colab_type": "code",
        "outputId": "36164f56-7739-4766-fae2-565a06b1af94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!mkdir -p ~/data/flowers\n",
        "!tar -xzf 102flowers.tgz -C ~/data/flowers/\n",
        "!rm 102flowers.tgz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
        "!mv setid.mat ~/data/flowers/setid.mat\n",
        "!mv imagelabels.mat ~/data/flowers/imagelabels.mat"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-03 09:39:23--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344862509 (329M) [application/x-gzip]\n",
            "Saving to: ‘102flowers.tgz’\n",
            "\n",
            "102flowers.tgz      100%[===================>] 328.89M  21.0MB/s    in 17s     \n",
            "\n",
            "2020-04-03 09:39:40 (19.8 MB/s) - ‘102flowers.tgz’ saved [344862509/344862509]\n",
            "\n",
            "--2020-04-03 09:39:52--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14989 (15K)\n",
            "Saving to: ‘setid.mat’\n",
            "\n",
            "setid.mat           100%[===================>]  14.64K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-04-03 09:39:52 (109 KB/s) - ‘setid.mat’ saved [14989/14989]\n",
            "\n",
            "--2020-04-03 09:39:55--  http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 502\n",
            "Saving to: ‘imagelabels.mat’\n",
            "\n",
            "imagelabels.mat     100%[===================>]     502  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-03 09:39:56 (97.0 MB/s) - ‘imagelabels.mat’ saved [502/502]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKfy3v40nwHX",
        "colab_type": "code",
        "outputId": "3ea194c6-07fa-42ca-825a-3a8b13fa6231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls ~/data/flowers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imagelabels.mat  jpg  setid.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_TIwcTgn0D_",
        "colab_type": "code",
        "outputId": "f7aa4928-03fe-4d65-d10b-0dd8a91643fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd ~/data/flowers/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/data/flowers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAZVtFzen5o2",
        "colab_type": "code",
        "outputId": "2463643e-3117-419c-a04b-8cfa25ce7da2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import scipy.io\n",
        "from keras.utils import to_categorical\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-9t6imixXNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_labels = scipy.io.loadmat(\"imagelabels.mat\")\n",
        "img_labels = img_labels[\"labels\"]\n",
        "img_labels = img_labels[0]\n",
        "for i in range(len(img_labels)):\n",
        "  img_labels[i] = img_labels[i] - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsKzHvw7xb2b",
        "colab_type": "code",
        "outputId": "597b85a5-c0b2-4567-9e1b-0f2fe4a36947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Number of labels:\",len(img_labels))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of labels: 8189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Sh2SMzxh4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_all = []\n",
        "labels_all = []\n",
        "dir = \"jpg/\"\n",
        "for imgs in os.listdir(dir):\n",
        "  img_num = int(imgs[7:11])-1\n",
        "  labels_all.append(img_labels[img_num])\n",
        "  image = cv2.imread(os.path.join(dir, imgs))\n",
        "  resized = cv2.resize(image, (150,150))\n",
        "  normalized_img = cv2.normalize(resized, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  images_all.append(normalized_img)\n",
        "images_all = np.array(images_all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXVzqdQwyeO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dividing train,test,val data in 60%,20%,20% respectively\n",
        "trainx, testx, trainy, testy = train_test_split(images_all, labels_all, test_size=0.2, random_state=10, stratify=labels_all)\n",
        "trainx, valx, trainy, valy = train_test_split(trainx, trainy, test_size=0.2, random_state=10,stratify=trainy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csdaxJka6BVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainy = to_categorical(trainy)\n",
        "testy = to_categorical(testy)\n",
        "valy = to_categorical(valy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJu92Ms6Cfb",
        "colab_type": "code",
        "outputId": "12e91869-3ca1-42b9-dd36-9c9f2608dc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(\"Training data number:\",len(trainx))\n",
        "print(\"Testing data number:\",len(testx))\n",
        "print(\"Validation data number:\",len(valx))\n",
        "print(\"Training labels number:\",len(trainy))\n",
        "print(\"Testing labels number:\",len(testy))\n",
        "print(\"Validation labels number:\",len(valy))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data number: 5240\n",
            "Testing data number: 1638\n",
            "Validation data number: 1311\n",
            "Training labels number: 5240\n",
            "Testing labels number: 1638\n",
            "Validation labels number: 1311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjYLBUqMxZij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, shape=[None, 150,150,3], name='x')\n",
        "y = tf.placeholder(tf.float32, shape=[None, 102], name='y')\n",
        "EPOCHS = 55\n",
        "BATCH_SIZE = 16\n",
        "PROB = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdK2a-ZTrC4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_weights(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
        " \n",
        "def create_biases(size):\n",
        "    return tf.Variable(tf.constant(0.05, shape=[size]))\n",
        "\n",
        "def create_convolutional_layer(input,\n",
        "               num_input_channels, \n",
        "               conv_filter_size,        \n",
        "               num_filters):  \n",
        "    \n",
        "    ## We shall define the weights that will be trained using create_weights function.\n",
        "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
        "    ## We create biases using the create_biases function. These are also trained.\n",
        "    biases = create_biases(num_filters)\n",
        " \n",
        "    ## Creating the convolutional layer\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                     filter=weights,\n",
        "                     strides=[1, 2, 2, 1],\n",
        "                     padding='SAME')\n",
        " \n",
        "    layer += biases\n",
        " \n",
        "    ## We shall be using max-pooling.  \n",
        "    layer = tf.nn.max_pool(value=layer,\n",
        "                            ksize=[1, 2, 2, 1],\n",
        "                            strides=[1, 2, 2, 1],\n",
        "                            padding='SAME')\n",
        "    ## Output of pooling is fed to Relu which is the activation function for us.\n",
        "    layer = tf.nn.relu(layer)\n",
        " \n",
        "    return layer\n",
        "\n",
        "\n",
        "def create_flatten_layer(layer):\n",
        "    layer_shape = layer.get_shape()\n",
        "    num_features = layer_shape[1:4].num_elements()\n",
        "    layer = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    return layer\n",
        "\n",
        "def create_fc_layer(input,          \n",
        "             num_inputs,    \n",
        "             num_outputs,\n",
        "             use_relu=True):\n",
        "    \n",
        "    #Let's define trainable weights and biases.\n",
        "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
        "    biases = create_biases(num_outputs)\n",
        "\n",
        "    layer = tf.matmul(input, weights) + biases\n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "    else:\n",
        "        layer = tf.add(tf.matmul(input, weights), biases, name='layer_fc2')\n",
        "\n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pwOSWKgyAfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "7c8a4926-3b62-46fd-a616-97dae669929d"
      },
      "source": [
        "layer_conv1 = create_convolutional_layer(input=x,\n",
        "               num_input_channels=3,\n",
        "               conv_filter_size=3,\n",
        "               num_filters=64)\n",
        " \n",
        "layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
        "               num_input_channels=64,\n",
        "               conv_filter_size=3,\n",
        "               num_filters=128)\n",
        " \n",
        "layer_conv3= create_convolutional_layer(input=layer_conv2,\n",
        "               num_input_channels=128,\n",
        "               conv_filter_size=3,\n",
        "               num_filters=256)\n",
        "          \n",
        "batch1 = tf.layers.batch_normalization(layer_conv3) \n",
        "drop1 = tf.nn.dropout(batch1, PROB)\n",
        "\n",
        "layer_flat = create_flatten_layer(drop1)[:2000]\n",
        " \n",
        "layer_fc1 = create_fc_layer(input=layer_flat,\n",
        "                     num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
        "                     num_outputs=1024,\n",
        "                     use_relu=True)\n",
        "\n",
        "batch2 = tf.layers.batch_normalization(layer_fc1)\n",
        "drop2 = tf.nn.dropout(batch2, PROB)\n",
        " \n",
        "layer_fc2 = create_fc_layer(input=drop2,\n",
        "                     num_inputs=1024,\n",
        "                     num_outputs=102,\n",
        "                     use_relu=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-a42318fa4ae7>:16: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-17-a42318fa4ae7>:17: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Uu85IqxKXT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6fe43f2b-a191-453c-8c3b-b613da670d95"
      },
      "source": [
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
        "                                                    labels=y)\n",
        "cost = tf.reduce_mean(cross_entropy)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(layer_fc2, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-3c0f374b7827>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuOcB2lu5pJR",
        "colab_type": "code",
        "outputId": "ca85f05a-11f5-4321-95fb-a31d921d9c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Training model\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(EPOCHS):\n",
        "        for batch in range(int(len(trainx)/BATCH_SIZE)):\n",
        "            batch_x = trainx[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(trainx))]\n",
        "            batch_y = trainy[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,len(trainy))]\n",
        "\n",
        "            opt = sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
        "\n",
        "        val_loss, val_acc = sess.run([cost, accuracy], feed_dict={x: valx, y : valy})\n",
        "        \n",
        "        print(\"Epoch\",epoch+1,\": Train loss=\",round(loss,4),\"Train accuracy=\",round(acc,4),\"| Valid loss=\",round(val_loss,4),\"Valid accuracy=\",round(val_acc,4))\n",
        "\n",
        "    #Save model\n",
        "    os.mkdir('/root/data/output')\n",
        "    tf.saved_model.simple_save(sess, '/root/data/output', inputs={\"x\": x}, outputs={\"layer_fc2\": layer_fc2})\n",
        "    print('--- SUCCESSFULLY SAVED MODEL ---')\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Train loss= 4.4609 Train accuracy= 0.0 | Valid loss= 4.4066 Valid accuracy= 0.0526\n",
            "Epoch 2 : Train loss= 3.9718 Train accuracy= 0.125 | Valid loss= 4.0289 Valid accuracy= 0.0801\n",
            "Epoch 3 : Train loss= 3.7087 Train accuracy= 0.0625 | Valid loss= 3.759 Valid accuracy= 0.1037\n",
            "Epoch 4 : Train loss= 3.3893 Train accuracy= 0.1875 | Valid loss= 3.6197 Valid accuracy= 0.1297\n",
            "Epoch 5 : Train loss= 3.5933 Train accuracy= 0.0625 | Valid loss= 3.4898 Valid accuracy= 0.1503\n",
            "Epoch 6 : Train loss= 3.4239 Train accuracy= 0.0625 | Valid loss= 3.3371 Valid accuracy= 0.1732\n",
            "Epoch 7 : Train loss= 3.0291 Train accuracy= 0.1875 | Valid loss= 3.2047 Valid accuracy= 0.2166\n",
            "Epoch 8 : Train loss= 3.2758 Train accuracy= 0.125 | Valid loss= 3.0638 Valid accuracy= 0.2265\n",
            "Epoch 9 : Train loss= 3.1064 Train accuracy= 0.1875 | Valid loss= 2.9751 Valid accuracy= 0.2471\n",
            "Epoch 10 : Train loss= 2.9797 Train accuracy= 0.125 | Valid loss= 2.8983 Valid accuracy= 0.2723\n",
            "Epoch 11 : Train loss= 2.7926 Train accuracy= 0.0625 | Valid loss= 2.831 Valid accuracy= 0.2738\n",
            "Epoch 12 : Train loss= 2.3589 Train accuracy= 0.375 | Valid loss= 2.7748 Valid accuracy= 0.2838\n",
            "Epoch 13 : Train loss= 2.5782 Train accuracy= 0.375 | Valid loss= 2.745 Valid accuracy= 0.3135\n",
            "Epoch 14 : Train loss= 2.2376 Train accuracy= 0.4375 | Valid loss= 2.6924 Valid accuracy= 0.3227\n",
            "Epoch 15 : Train loss= 2.2776 Train accuracy= 0.375 | Valid loss= 2.6143 Valid accuracy= 0.3394\n",
            "Epoch 16 : Train loss= 2.3179 Train accuracy= 0.4375 | Valid loss= 2.613 Valid accuracy= 0.3242\n",
            "Epoch 17 : Train loss= 2.4092 Train accuracy= 0.375 | Valid loss= 2.6289 Valid accuracy= 0.315\n",
            "Epoch 18 : Train loss= 2.3751 Train accuracy= 0.375 | Valid loss= 2.5571 Valid accuracy= 0.3272\n",
            "Epoch 19 : Train loss= 2.0154 Train accuracy= 0.3125 | Valid loss= 2.4786 Valid accuracy= 0.3616\n",
            "Epoch 20 : Train loss= 1.9036 Train accuracy= 0.5625 | Valid loss= 2.4401 Valid accuracy= 0.3677\n",
            "Epoch 21 : Train loss= 2.0047 Train accuracy= 0.5625 | Valid loss= 2.4637 Valid accuracy= 0.357\n",
            "Epoch 22 : Train loss= 2.4276 Train accuracy= 0.4375 | Valid loss= 2.3854 Valid accuracy= 0.3745\n",
            "Epoch 23 : Train loss= 1.5996 Train accuracy= 0.4375 | Valid loss= 2.4734 Valid accuracy= 0.3715\n",
            "Epoch 24 : Train loss= 1.535 Train accuracy= 0.625 | Valid loss= 2.4429 Valid accuracy= 0.3654\n",
            "Epoch 25 : Train loss= 1.8361 Train accuracy= 0.625 | Valid loss= 2.4455 Valid accuracy= 0.3814\n",
            "Epoch 26 : Train loss= 2.0146 Train accuracy= 0.4375 | Valid loss= 2.4413 Valid accuracy= 0.3844\n",
            "Epoch 27 : Train loss= 1.418 Train accuracy= 0.625 | Valid loss= 2.3411 Valid accuracy= 0.4058\n",
            "Epoch 28 : Train loss= 1.8092 Train accuracy= 0.625 | Valid loss= 2.3767 Valid accuracy= 0.4035\n",
            "Epoch 29 : Train loss= 2.0196 Train accuracy= 0.5625 | Valid loss= 2.4187 Valid accuracy= 0.4104\n",
            "Epoch 30 : Train loss= 1.5564 Train accuracy= 0.4375 | Valid loss= 2.3271 Valid accuracy= 0.4195\n",
            "Epoch 31 : Train loss= 1.5115 Train accuracy= 0.5 | Valid loss= 2.3886 Valid accuracy= 0.3944\n",
            "Epoch 32 : Train loss= 1.5966 Train accuracy= 0.6875 | Valid loss= 2.4165 Valid accuracy= 0.3997\n",
            "Epoch 33 : Train loss= 1.2235 Train accuracy= 0.625 | Valid loss= 2.3612 Valid accuracy= 0.4119\n",
            "Epoch 34 : Train loss= 1.8874 Train accuracy= 0.375 | Valid loss= 2.3744 Valid accuracy= 0.4081\n",
            "Epoch 35 : Train loss= 1.4339 Train accuracy= 0.5625 | Valid loss= 2.5152 Valid accuracy= 0.3844\n",
            "Epoch 36 : Train loss= 1.3528 Train accuracy= 0.5625 | Valid loss= 2.4343 Valid accuracy= 0.4256\n",
            "Epoch 37 : Train loss= 0.5802 Train accuracy= 0.875 | Valid loss= 2.4677 Valid accuracy= 0.4096\n",
            "Epoch 38 : Train loss= 1.4404 Train accuracy= 0.6875 | Valid loss= 2.4514 Valid accuracy= 0.4157\n",
            "Epoch 39 : Train loss= 1.6797 Train accuracy= 0.5625 | Valid loss= 2.4621 Valid accuracy= 0.4104\n",
            "Epoch 40 : Train loss= 0.6339 Train accuracy= 0.875 | Valid loss= 2.4881 Valid accuracy= 0.4165\n",
            "Epoch 41 : Train loss= 1.2567 Train accuracy= 0.75 | Valid loss= 2.525 Valid accuracy= 0.4241\n",
            "Epoch 42 : Train loss= 0.6695 Train accuracy= 0.8125 | Valid loss= 2.4903 Valid accuracy= 0.4058\n",
            "Epoch 43 : Train loss= 0.905 Train accuracy= 0.75 | Valid loss= 2.5449 Valid accuracy= 0.405\n",
            "Epoch 44 : Train loss= 0.9331 Train accuracy= 0.75 | Valid loss= 2.4252 Valid accuracy= 0.4371\n",
            "Epoch 45 : Train loss= 0.9087 Train accuracy= 0.6875 | Valid loss= 2.5138 Valid accuracy= 0.4226\n",
            "Epoch 46 : Train loss= 0.575 Train accuracy= 0.75 | Valid loss= 2.5838 Valid accuracy= 0.4256\n",
            "Epoch 47 : Train loss= 0.8469 Train accuracy= 0.6875 | Valid loss= 2.6461 Valid accuracy= 0.4294\n",
            "Epoch 48 : Train loss= 0.7803 Train accuracy= 0.6875 | Valid loss= 2.5993 Valid accuracy= 0.4241\n",
            "Epoch 49 : Train loss= 0.6071 Train accuracy= 0.8125 | Valid loss= 2.5952 Valid accuracy= 0.4165\n",
            "Epoch 50 : Train loss= 1.1087 Train accuracy= 0.75 | Valid loss= 2.673 Valid accuracy= 0.4165\n",
            "Epoch 51 : Train loss= 0.6756 Train accuracy= 0.8125 | Valid loss= 2.6432 Valid accuracy= 0.4256\n",
            "Epoch 52 : Train loss= 0.6944 Train accuracy= 0.6875 | Valid loss= 2.5567 Valid accuracy= 0.4462\n",
            "Epoch 53 : Train loss= 0.8955 Train accuracy= 0.75 | Valid loss= 2.6566 Valid accuracy= 0.4355\n",
            "Epoch 54 : Train loss= 0.4751 Train accuracy= 0.75 | Valid loss= 2.5955 Valid accuracy= 0.4218\n",
            "Epoch 55 : Train loss= 0.595 Train accuracy= 0.8125 | Valid loss= 2.6863 Valid accuracy= 0.4325\n",
            "WARNING:tensorflow:From <ipython-input-19-40b8f0145fc5>:19: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: /root/data/output/saved_model.pb\n",
            "--- SUCCESSFULLY SAVED MODEL ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh7P-HJZRCW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}